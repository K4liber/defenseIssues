Entropia jest to termodynamiczna funkcja stanu, określająca kierunek przebiegu procesów spontanicznych (samorzutnych) w odosobnionym układzie termodynamicznym. Entropia jest miarą stopnia nieuporządkowania układu i rozproszenia energii. Jest wielkością ekstensywną. Zgodnie z drugą zasadą termodynamiki, jeżeli układ termodynamiczny przechodzi od jednego stanu równowagi do drugiego, bez udziału czynników zewnętrznych (a więc spontanicznie), to jego entropia zawsze rośnie. W ramach II zasady termodynamiki zmiana entropii (w procesach kwazistatycznych) jest zdefiniowana przez swoją różniczkę zupełną jako:\newline
$ dS = \frac{1}{T}\delta Q $, gdzie:\newline
$ T $ - temperatura bezwzględna,\newline
$ \delta Q $ - ciepło elementarne, czyli niewielka ilość ciepła dostarczonego do układu.\newline
Entropię pewnego stanu termodynamicznego P można wyznaczyć ze wzoru:\newline
$ S(P) = \int_{0}^{T_p}\frac{C(T)dT}{T} $, gdzie:\newline
$ C(T) $ - pojemność cieplna (ilość ciepła, jaka jest niezbędna do zmiany temperatury ciała o jednostkę temperatury),\newline
$ T_p $ - temperatura w stanie P.
Podstawowe równanie termodynamiki fenomenologicznej, w którym występuje entropia, ma postać:\newline
$ dU = TdS - pdV + \sum_{i=1}^{k}\mu_idN_i $, gdzie:\newline
$ U $ - energia wewnętrzna,\newline
$ k $ - liczba różnych składników,\newline
$ T $ - temperatura,\newline
$ p $ - ciśnienie,\newline
$ \mu_i $ - potencjał chemiczny i-tego składnika.

W termodynamice statystycznej całkowita entropia układu makroskopowego jest równa:\newline
$ S = k\ln(W) $\newline
lub\newline
$ S = -k\sum_{i}p_i\ln(p_i) $, gdzie:\newline
$ k $ - stała Boltzmanna,\newline
$ W $ - liczba sposobów, na jakie makroskopowy stan termodynamiczny układu (makrostan) może być zrealizowany poprzez stany mikroskopowe (mikrostany),\newline
$ p_i $ - prawdopodobieństwo i-tego mikrostanu.\newline
Zatem $ \log_2(W) $ jest liczbą bitów potrzebnych do określenia, którą realizację przyjął dany układ.

